# Disaster Response Pipeline Project
## Project Overview

Through the [Udacity - Data Scientist Nanodegree Program](https://www.udacity.com/course/data-scientist-nanodegree--nd025), I've honed and expanded my data engineering skills, opening up new opportunities and potential as a data scientist. This project leverages these skills to analyze disaster data sourced from [Figure Eight](https://www.figure-eight.com/), with the aim of building a model for a disaster-messaging API.

The project's dataset comprises authentic messages sent during disaster events. I will construct a machine learning pipeline to categorize these messages and ensure timely communication with the relevant disaster relief organizations.

As a part of this project, I will develop a web application. This application will enable emergency responders to input new messages and receive classifications across multiple categories. Additionally, the data will be presented visually through the web application.
## File structure
```
- app
| - template
| |- master.html  # main page of web app
| |- go.html  # classification result page of web app
|- run.py  # Flask file that runs app

- data
|- disaster_categories.csv  # data to process 
|- disaster_messages.csv  # data to process
|- process_data.py
|- DisasterResponse.db   # database to save clean data to

- models
|- train_classifier.py
|- classifier.pkl  # saved model 

- README.md
```
## Project Components

This project consists of three main components:

### 1. ETL Pipeline

In the `process_data.py` Python script, we implement an ETL (Extract, Transform, Load) pipeline that performs the following tasks:

- Loads the `messages` and `categories` datasets.
- Merges the two datasets into a single cohesive dataset.
- Cleans and preprocesses the data to make it suitable for analysis.
- Stores the cleaned data in a SQLite database for easy retrieval and use.

### 2. ML Pipeline

In the `train_classifier.py` Python script, we create a machine learning pipeline that includes the following steps:

- Loads the data from the SQLite database generated by the ETL pipeline.
- Splits the dataset into training and test sets for model evaluation.
- Constructs a text processing and machine learning pipeline to handle the data.
- Trains and fine-tunes a machine learning model using GridSearchCV to achieve optimal performance.
- Evaluates and reports the model's performance on the test set.
- Exports the final trained model as a pickle file for future use.

### 3. Flask Web App

This component involves building a web application using Flask, which allows users to interact with the trained model. Key features of the web app include:

- Customization of file paths for the database and model as needed.
- Integration of data visualizations using Plotly to present informative insights and results through the web interface.

### Instructions:
1. Run the following commands in the project's root directory to set up your database and model.

    - To run ETL pipeline that cleans data and stores in database
        `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
    - To run ML pipeline that trains classifier and saves
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`

2. Go to `app` directory: `cd app`

3. Run your web app: `python run.py`

4. Click the `PREVIEW` button to open the homepage
5. Link Git : https://github.com/TinTran13/Udacity_DS_Project2
